\chapter{設計}
本章では，提案手法を実現するための設計について述べる．
本研究では提案手法を実現するために，DDB として MongoDB を，
VMM として QEMU/KVM を採用する．

本提案機構は，次の 3 要素から構成される．
\begin{itemize}
  \item キャッシュクリア機構
  \item MongoDB・QEMU 間の連携機構
  \item キャッシュ復元機構
\end{itemize}
キャッシュクリア機構は，データベースとしてのデータの整合性を保証しつつ，
最大限のメモリ削減効果を得るようにキャッシュの解放を行う，
MongoDB・QEMU 間の連携機構は，セマンティクスギャップを解消するためのアドレス変換と，
それによって構築されたビットマップによる転送制御を行う．
キャッシュ復元機構は，移送完了後の MongoDB 上のキャッシュの透過的な復元を行う．
以降はそれぞれの機構の設計について述べる．


\section{キャッシュクリア機構}
本節では，提案手法におけるキャッシュクリア機構の設計について述べる．
キャッシュクリア機構はライブ移送の開始直前に動作し，
MongoDB 上のキャッシュの解放を行う．
この際，データベースとしてのデータの整合性を保証しつつ，
最大限のメモリ削減効果を得るようにすることが求められる．


\subsection{チェックポイント処理によるデータ整合性の保証}
4.3.1 節で述べたように，MongoDB 上のキャッシュ領域を安全に解放するには，
更新データの整合性保証が不可欠である．
そのため本提案機構では，キャッシュ解放プロセスの第一段階としてチェックポイント処理を実行する．
この処理によって，MongoDB 上のキャッシュ内に存在する更新データを一括でストレージに反映させ，
全ページをクリーンな状態にすることができる．
そのため，解放フェーズではストレージへの書き込みを伴わない高速なキャッシュ解放が可能となる．

\subsection{B+Tree 構造を考慮したキャッシュクリア}
MongoDB 上のキャッシュ領域を安全に解放するためには，
解放前後でデータ構造の依存関係を維持している必要がある．
そこで本提案機構では，キャッシュの解放対象を B+Tree のリーフページに限定し，
内部ページはキャッシュ上に残す方針をとる．
MongoDB のストレージエンジンである WiredTiger は
B+Tree 構造に基づいてキャッシュを管理しており，実データはリーフページにのみ存在する一方，
内部ページには参照ポインタなどのメタデータが格納されている．
そのため，内部ページを解放してもメモリ削減効果は小さいにもかかわらず，
データ構造の整合性を損なう可能性があるため，これらを解放対象から除外する．

この方針を実現するために，WiredTiger のキャッシュ解放プロセスに変更を加える．
キャッシュクリア機構の処理フローを図\ref{fig:cache_clear_flow}に示す．
このプロセスは，B+Tree を走査して候補ページを退避キューに収集するフェーズと
退避キュー内のページを実際に解放するフェーズの 2 段階で構成されており，
収集フェーズにおいてはリーフページのみを退避キューに入れるように変更し，
解放フェーズにおいては探索範囲の最適化を無効化するようにする．

まず，収集フェーズについて述べる．
通常の収集フェーズでは，ページの種類を区別せずに退避キューに入れるが，
本提案機構ではリーフページのみを退避キューに追加し，内部ページは除外するようにする．
これにより，解放フェーズが実行される前に解放対象外のページを確実に除外しつつ，
限られたキューの容量をリーフページのみで満たすことで，一度の解放サイクルの処理効率を高める．

解放フェーズでは，キャッシュ解放のためのページ操作において探索範囲の最適化を無効化し，
退避キューに含まれる全てのページに対して解放を行うようにする．
通常のデータベースのキャッシュ退避機構には，性能維持のために
一定数の候補が見つかったり，走査回数が閾値を超えたりした場合に
探索を打ち切るヒューリスティックな最適化が含まれている．
しかし移送時において，この最適化はキャッシュ解放の効果を減少させ，
メモリ転送量の削減効果を損なってしまう．
そのためこれらの探索制限を撤廃し，
キャッシュ内の全ての対象リーフページに対して探索と解放操作を行う．

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\linewidth]{figure/cache_clear_flow.png}
  \caption{キャッシュクリア機構の処理フロー}
  \label{fig:cache_clear_flow}
\end{figure}


\section{MongoDB・QEMU 間の連携機構}
本節では，MongoDB と QEMU の間で解放したキャッシュ情報を共有するための連携機構について述べる．
ゲスト OS 上で動作する MongoDB とホスト OS 上で動作する QEMU は，
それぞれ異なるアドレス空間とメモリ管理機構を持つ．
そのため本機構では，セマンティクスギャップを解消するためのアドレス変換と，
それによって共有された情報に基づいた転送制御を行う．

\subsection{MongoDB 側におけるアドレス変換と通知データ構築}
MongoDB 側では，解放したキャッシュにおいてメモリ転送量の削減効果が大きいメモリ領域を選定し，
それらを QEMU が解釈可能な形式へと変換・集約する役割を担う．

アドレス変換の概要を図\ref{fig:address_transrate}に示す．
本提案機構では，ゲスト仮想アドレス (GVA) からゲスト物理アドレス (GPA) への変換を MongoDB 側で行う．
GVA はゲスト OS 上の各プロセスが持つ仮想アドレス空間でのみ有効な値であり，一意性が保証されない．
QEMU 側で変換を行う場合，通知された GVA がどのプロセスかを適切に特定するために，
ゲスト OS 内で現在どのプロセスが実行状態にあるかを常に追跡する必要があり，
これは実装の複雑化や誤変換のリスクを伴う．
そのため，自身のページテーブルへ安全かつ確実にアクセスできる MongoDB 側で GPA への変換を行い，
一意な GPA を QEMU に通知することで，セマンティクスギャップを解消する．

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\linewidth]{figure/address_transrate.png}
  \caption{アドレス変換の概要図}
  \label{fig:address_transrate}
\end{figure}

\subsection*{WiredTiger の構造に基づく変換対象アドレスの選定}
まず，変換対象とするアドレスの選定方針について述べる．
キャッシュ解放による転送量の削減効果を最大化するため，
本提案機構ではキャッシュ解放処理の実行時に，
解放対象ページのベースイメージと更新データリストを変換対象とし，
それぞれに対応する GVA を網羅的に取得する．
%2.3.2 節で述べたように，
WiredTiger のリーフページ内では，
ディスクから読み込んだベースイメージと，そのデータへの変更履歴を保持する更新データリスト
の 2 つに分けて管理されており，
特に書き込み負荷が高い環境下では，更新データがメモリを占める割合が大きくなる．
そのため，転送量を大幅に削減するには，
ベースイメージだけでなく更新データの転送スキップが不可欠である．
また，更新データは連結リスト構造として不連続なメモリ領域に配置されているため，
リストを順次走査して更新データに対応する全ての GVA を取得する．

\subsection*{多段リスト構造を用いた通知データの管理}
次に，GPA に変換したアドレスの QEMU への通知方法について述べる．
本提案機構では変換した GPA を 3 層からなる多段リスト構造として管理し，
そのルートアドレスのみを QEMU に通知する．
多段リスト構造を図\ref{fig:Multi_level_List}に示す．
このリストは 3 階層からなり，最下層に変換を行った GPA を格納した配列が配置され，
中間層にその配列の先頭アドレスを格納した配列が配置され，
最上位層に中間層の先頭アドレスを格納した配列が配置される．
このような階層構造を採用した理由は，大きく分けて 2 つある．

第 1 の理由は，メモリ領域確保の確実性である．
単一の配列で大量の GPA を管理しようとした場合，
巨大な連続したメモリ領域を確保する必要があり，
外部断片化によりメモリ確保の失敗を招くリスクがある．
しかし多段リスト構造では，ページ単位に分散した空きメモリ領域を
論理的に連結して利用することができるため，メモリ断片化が進んでいる状況であっても，
必要なメモリ領域を柔軟かつ確実に確保することができる．

第 2 の理由は，通知処理および参照処理の高速化である．
本提案機構では，QEMU への通知を I/O ポートを経由で行うが，
この操作はゲストモードからホストモードへの遷移を伴うコストの高い操作であるため，
最小限の回数で抑えることが求められる．
仮に外部断片化を避けるためにデータを小さな配列に分割して管理する場合，
一度に格納できるアドレス数が減少し，その都度 I/O ポートによる通知を要するため，
I/O オーバヘッドが増加してしまう．
一方連結リスト構造を用いた場合は，I/O ポート回数を最小限に抑えることはできるが，
QEMU 側でのアドレス参照にボトルネックが生じる．
連結リストはあるノードを読み出さなければ次のノードのアドレスが判明しないため，
ポインタ追跡によるメモリアクセスの直列化が発生する．
さらに各ノードがメモリ上に分散するため，ホストからゲストメモリへの読み取り操作の回数が増大し，
多大なオーバーヘッドとなる．
これに対し多段リスト構造は，ルートアドレスの通知という最小限の I/O 回数で
大量のアドレスを QEMU に通知しつつ，
ページ内の連続したデータを一括で読み取ることで空間局所性を活かし，
通信と参照の双方において高速性を実現している．

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{figure/Multi_level_List.png}
  \caption{多段リスト構造}
  \label{fig:Multi_level_List}
\end{figure}


\subsection{QEMU 側における通知アドレスの受信と転送制御}
QEMU 側では，通知された GPA アドレスをもとに，該当するページの転送をスキップする役割を担う．
本提案機構では，通知されたアドレスを非同期的に転送スキップを判定するビットマップに反映させ，
ライブ移送の際には，そのビットマップをもとに転送スキップの判定を行う．

\subsection*{非同期的なアドレス登録処理}
本提案機構では通知の受信処理とアドレス登録処理を分離し，非同期的に実行する．
具体的には，MongoDB からの I/O ポートによる通知を検知したら，
QEMU のハンドラはアドレス登録処理のリクエストを内部キューに追加して
即座にゲスト OS に制御を戻す．キューに追加されたアドレス登録処理は，
QEMU のメインイベントループが管理する非同期タスクとしてバックグラウンドで実行される．
これにより登録処理を vCPU スレッドから分離して，
ホスト OS 側の CPU リソースを用いて実行させることで，
QEMU への通知に伴うオーバヘッドを最小化し，VM 内の性能劣化を最小限に抑える．

\subsection*{アドレス解析およびスキップ用ビットマップへの登録}
この処理では，MongoDB から受け取ったスキップ対象となるページの GPA を，
QEMU 内部のメモリ管理構造と照合し，スキップ用ビットマップに登録する．

その第一段階として，受け取った GPA を QEMU が認識できるホスト仮想アドレス (HVA) に変換する．
ゲスト OS が認識している GPA はゲスト物理メモリ空間の値であり，
QEMU の仮想アドレス空間とは独立している．
そのため，QEMU は GPA をそのまま用いて対象のメモリ領域へアクセスすることはできない．
したがって QEMU が管理するメモリマッピング情報に基づき HVA に変換することで，
対象ページへのアクセスと後述するメモリブロックの特定を可能にする．

続いて，取得した HVA をもとに，そのページが属するメモリ管理ブロックと
ブロック内オフセットを特定する．
QEMU の移送機構は，メモリ領域を分割されたブロック単位として管理しているため，
HVA がどのブロックのどの位置に対応しているか変換する必要がある．

そして，特定したブロック内オフセットからビットマップ上のインデックスを求め，
スキップ用ビットマップ内の該当ビットをセットする．
これらの処理を通知された全 gpa に対して行うことで，
転送可否を判定するスキップ用ビットマップを構築する．
これにより，転送フェーズにおいて各ページの転送可否を，
定数時間の計算量で高速に判定することが可能となる．

\subsection*{スキップ用ビットマップを用いた転送判定}
QEMU のライブ移送における反復転送フェーズでは，
ダーティビットマップを参照し，直前の転送以降に変更が生じたページのみを転送対象とする．
本提案機構ではこの判定に加え，転送対象として選ばれたページに対し，
さらにスキップ用ビットマップでの判定を行う．
例えば，あるページがダーティと判定されて転送対象と選ばれた場合であっても，
スキップ用ビットマップ上の該当ビットが有効であれば，そのページの転送を行わない．
この判定機構により，MongoDB 側でキャッシュが解放され不要となったデータの
転送を削減することが可能となる．
この判定はビットマップの参照のみで完結するため，
転送ループの処理遅延を最小限に抑えつつ，総転送量の削減を実現する．


\section{キャッシュ復元機構}
本節では，転送をスキップしたメモリページを移送先ホストにおいて適切に処理し，
MongoDB から見て透過的な状態へ復元するための設計について述べる．

本提案機構では，移送プロトコルのヘッダ情報にスキップフラグを新たに定義し，
メモリの転送の明示的な省略処理を実現する．
既存の QEMU 内の移送管理機構には，ページの転送をスキップしたことを移送先に通知する仕組みが存在しない．
そのため単にデータ本体を送信しないだけでは，受信側で転送がスキップされたのか，
通信障害によるデータ欠落なのかを区別できず，メモリ不整合や移送失敗を引き起こすリスクがある．
そこで移送元において，転送をスキップと判定したページに対して，
ヘッダ情報にスキップフラグを設定して移送先に通知する．
これにより受信側に対して，該当ページの転送を意図的に省略したことを通知する．

移送先ホストではスキップフラグを受信したページに対して書き込みを行わずにゼロページとして維持し，
データ復元は移送完了後の WiredTiger が標準的なキャッシュミス処理によって透過的に行う．
5.1 節で述べたように，B+Tree の内部ページは解放しないため，
データ構造のメタデータは移送先ホストに正常に転送されている．
この際，内部ページが保持する参照ポインタは，リーフページがキャッシュ上に存在しないことを示している．
そのため，移送完了後に MongoDB が該当ページにアクセスを要求した際，
WiredTiger はメタデータに基づいて該当ページがキャッシュ上にないことを検出する．
そして，通常のキャッシュミス処理として処理され，共有ストレージから該当データが再取得される．
これにより，MongoDB が移送後に特別な復元処理を行うことなく，透過的にデータを復元できる．
