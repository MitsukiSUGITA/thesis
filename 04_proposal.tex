\chapter{提案}

\section{概要}
2.4 節で述べたように，MongoDB のような DDB を実行している VM のライブ移送では，
メモリサイズやダーティレートが移送の性能に大きな影響を与える．
特に，Pre-copy 手法においてダーティレートが転送速度を上回る場合，
反復転送が収束せずにダウンタイムの増大やサービス品質の低下を招く点が課題である．

そこで，本研究では DDB が稼働している仮想マシンを対象としたライブ移送を高速化する手法を提案する．
提案手法では，VM のメモリサイズやダーティレートがライブ移送の性能に
影響を与える主要因の一つであることに着目し，
DDB が保持するキャッシュを解放してそれらを転送対象から除外し，
従来の移送手法では転送されていた DDB 上のメモリ領域を削減することで，移送時間の短縮を図る．
本研究では共有ストレージ環境を前提としており，移送先 VM では転送をスキップした DDB 上の
キャッシュを共有ストレージから読み込むことで再構築を行う．
これにより DDB のメモリ使用量が大きく，ダーティレートが高い状況下でも
転送すべきデータ量を少量に保つことができ，
総移送時間とダウンタイム双方の短縮を実現する．
その結果 DDB が稼働している VM においても短時間での移送を実現し，
ライブ移送による負荷分散や電力削減といった効果の享受を可能にする．

\section{アプローチ}
提案手法の概略図を図\ref{fig:Post_copy}に示す．
本研究では信頼性が高い Pre-copy 手法を基に，VM 内の DDB アプリケーションと
 VMM が連携して移送の効率化を図る．具体的な手順としてはまず，
移送を開始する直前に DDB が自身のメモリ管理機構を用いてキャッシュの解放を行う．
このとき，解放したページのアドレス情報を VMM に通知する．
移送元ホストの VMM は，受け取ったページ情報をもとにページの転送可否を判定するデータ構造を作成する．
実際のライブ移送中において VMM はこの情報を参照し，DDB 上のキャッシュ領域の転送をスキップする．
そして，移送先ホストにおける VM の実行状態は，移送元ホストからのネットワーク転送と
共有ストレージからのストレージアクセスを組み合わせて復元される．

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\linewidth]{figure/Proposal.png}
  \caption{提案手法の概略図}
  \label{fig:Proposal}
\end{figure}

\section{Design Challenges}
提案手法を実現する上で解決すべき，設計上の課題を述べる．

\subsection*{DDB の内部状態を考慮した安全なキャッシュ解放}%(→ 先にストレージにフラッシュする)
%\subsubsection*{どのようにして DDB 上の安全にデータキャッシュを解放するか}%(→ 先にストレージにフラッシュする)
%ここは， 1 パラグラフ 1 トピックが守られているか分からない．問題提起->課題なのか問題点->技術的課題とするか
DDB のキャッシュ領域には，ディスクと同期済みであるクリーンページ，
ページ内容が更新されたダーティページ，インデックスなどのメタデータが混在している．
ゲスト OS や VMM などの DDB 外部からはこれらのページのセマンティクスを理解していないため，
無差別に DDB 上のキャッシュを解放すると DDB 内部の整合性を破壊し，
プロセスのクラッシュやデータ損失を引き起こすリスクがある．
したがって，DDB 内部から自身のキャッシュ状態を判断し，
安全に解放する必要がある．これを実現するにあたり 2 つの課題が存在する．

 1 つ目は，更新データの整合性保証である．
ダーティページを単にメモリから解放した場合，変更内容はディスク上に反映されないため，
移送先ホストでそのデータを読み込んだ際に，直近の更新内容が消失してしまう．
そのため，解放するメモリ上の更新内容を共有ストレージに反映させたうえで，解放を行う必要がある．
しかし，ダーティページを個別に解放しようとすると，その都度ディスクへの書き戻しが発生し，
多大な I/O オーバーヘッドによって移送時間の短縮効果が相殺されてしまう．
したがって，効率的に更新データの整合性を保証しつつ，安全にキャッシュを解放することが求められる．

 2 つ目は，データ構造の依存関係の維持である．
例えば B+Tree などの DDB 内のキャッシュ管理構造において，内部ノードを誤って解放すると
リーフノードへのアクセスが不可能になり，データベースが機能しなくなる．
そのため，依存関係に影響を及ぼさないページのみを選定して解放しなければならない．


\subsection*{どのようにしてデータキャッシュのページ転送をスキップするか}%（->DB-OS-Hypervisor を連動させてアドレスを変換する，メモリの反復転送の際に転送の可否を判定）
%\subsubsection*{どのようにしてデータキャッシュのページ転送をスキップするか}%（->DB-OS-Hypervisor を連動させてアドレスを変換する，メモリの反復転送の際に転送の可否を判定）

本提案機構は，DDB と VMM 間のメモリ管理におけるセマンティックギャップを解消し，
VMM のメモリ転送機構に低遅延で統合することが求められる．

これを実現するにあたり，まず DDB と VMM 間アドレスの不一致を解消する必要がある．
VM 上で動作する DDB はゲスト仮想アドレス (GVA) でメモリを管理する一方，
VMM はゲスト物理アドレス (GPA) やホスト仮想アドレス (HVA)，内部のメモリ管理構造を用いてメモリの管理を行っている．
DDB が扱う GVA はそのプロセス内でのみ有効な値であり，VMM がこれを直接解釈することはできない．
もし，未変換のアドレスを VMM でそのまま使用してしまうと，
カーネル領域などの誤ったメモリ領域をスキップ対象と誤認し，データが破損する恐れがある．
そのため，DDB と VMM 間で連携するには，アドレス変換を行ってセマンティクスギャップを埋めることが求められる．

また，VMM のメモリ転送機構に対して，オーバヘッドを最小限に抑えつつ転送スキップ処理を統合することが求められる．
キャッシュを解放したページアドレスのリストを DDB から適切に受信しても，
ライブ移送中のメモリ転送の際に，DDB から通知された膨大な転送スキップリストで毎回判定すると，
転送ループごとの判定コストが増加するため，転送スループットの低下や総移送時間が長期化を招く．
そのため，通知されたリストを VMM でビットマップ等の瞬時に参照可能な形式に変換し，
それを用いてスキップ判定を行うことで，移送を効率化する必要がある．

%なぜ解放するだけでなく，qemuにアドレスを通知して転送をスキップする必要があるのか？
%qemuはmongoのキャッシュを理解していないため，mongoDBで解放してもqemu側ではまだ必要だと思っている

%QEMUはゲストの物理アドレスしか理解できないため、MongoDBはキャッシュをクリアする際、対象となるページの仮想アドレス（GVA）を物理アドレス（GPA）に変換が必須となります。

\subsection*{どのようにしてストレージから復元するか}%(-> DDB のセマンティクスを利用してファイルから該当データを取得する)
%\subsubsection*{どのようにしてストレージから復元するか}%(-> DDB のセマンティクスを利用してファイルから該当データを取得する)
 3 つ目の課題は，転送をスキップしたページが移送先ホストで適切にアクセス可能であり，
アプリケーションから見て透過的に復元されることを保証することである．
本手法では，DDB 上のキャッシュ部分のメモリページを転送しないため，
移送先ホストでは該当ページが転送を正常にスキップされたことを認識する必要がある．
また，該当部分は移送先ホストの VM 上に存在しない．そのため，
移送先で DDB が該当データにアクセスした際，DDB のセマンティクスを利用して
自動的に共有ストレージから該当データを取得する必要がある．
