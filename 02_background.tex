\chapter{背景}

\section{仮想化技術}
仮想化技術とは物理マシンをソフトウェアによってエミュレートし，仮想化されたリソースを用いて
自身の OS やアプリケーションを物理マシンから独立した環境で仮想マシン (VM) として
動作させる技術である．VM は単一の物理マシン上で複数構築することが可能であり，
異なる OS を同時に動作させることができる．仮想マシンモニタ (VMM) という
ソフトウェアによって VM の作成，管理が行われる．


\subsection{仮想マシンモニタ}
VMM はハイパーバイザとも呼ばれ，物理マシン上で動作して CPU やメモリ等の
ハードウェアリソースを VM に割り当てる．ソフトウェアとしての VMM は，
 3 つの本質的な特徴を持っている．
1 つ目の特徴は等価性であり， 仮想環境で実行されるプログラムは物理マシン上で実行される場合と
同等の結果を示す．ホストマシンより少ないメモリ量を割り当てられた VM 上であっても，
計算結果はホストマシン上での実行時と論理的に一致する必要がある．
2 つ目は効率性であり， VM 上で実行されるプログラムの大部分は直接ハードウェア上で実行される．
特権を必要としない演算命令などは VMM が解釈して実行するのではなく，
ハードウェアのプロセッサが直接処理する．
そして 3 つ目はリソース制御であり，VMM は VM に割り当てたリソースを完全に制御する．
メモリや I/O デバイスに対して割り当てた領域以外へのアクセスを防いだり，
一度割り当てていても，状況に応じて VMM がそれらの制御を取り戻すことが可能である必要がある\cite{popekFormalRequirementsVirtualizable1974}．

VMM には，動作させる方法によって大きく分けてハイパーバイザ型とホスト OS 型の
 2 種類に分類される．
ハイパーバイザ型の構成を図\ref{fig:hypervisor}に示す．
ハイパーバイザ型は， VMM をハードウェアに直接インストールして動作させる方式である.
ハードウェアリソースに直接アクセスできるため VM を効率的に動作させることがでる．
またハードウェア上で動作する VM を管理する小さなプログラムであるため，
攻撃対象領域が最小限に抑えられ，高いセキュリティを実現する\cite{aalamReviewPaperHypervisor2021}．
ホスト OS 型の構成を図\ref{fig:host_os}に示す．
ホスト OS 型は，VMM が物理マシン上の既存の OS 上でアプリケーションとして動作する方式である．
インストールが容易であることが利点だが，VM がハードウェアにアクセスする場合に 
VMM やホスト OS を仲介する必要があるため，ハイパーバイザ型と比べてレイテンシが高い．
またホスト OS 上で動作するため，ホスト OS の脆弱性を継承する．
ホスト OS が侵害されると，その上で動作する VMM や VM も制御を失うリスクがあるため，
ハイパーバイザ型と比べてセキュリティが脆弱になる\cite{aalamReviewPaperHypervisor2021}．

\begin{figure}[H]
  \centering
  % --- 左側の図 ---
  \begin{minipage}[t]{0.3\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figure/hypervisor.png}
    \caption{ハイパーバイザ型の構成}
    \label{fig:hypervisor} % ラベル名は変えておく
  \end{minipage}
  \hspace{20mm} % 図と図の間に隙間を空ける
  % --- 右側の図 ---
  \begin{minipage}[t]{0.3\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figure/host_os.png}
    \caption{ホスト OS 型の構成}
    \label{fig:host_os} % ラベル名は変えておく
  \end{minipage}
\end{figure}

\subsection{仮想化手法}

仮想化手法は，ゲスト OS に対する変更の有無で完全仮想化と準仮想化に分類することができる．
完全仮想化は，VMM がハードウェアの環境をソフトウェアでエミュレートすることで，
ゲスト OS を変更せずに仮想化を実現する手法である\cite{barhamXenArtVirtualization} ．
ゲスト OS に変更を加えないため互換性が高く，様々な OS を動作させることができ，
ゲスト OS は仮想化されていることを認識しない．
しかし，VM 内でハードウェアリソースへのアクセスを要する特権命令が実行されると，
トラップが発生して VMM に制御が移り，VMM がハードウェア操作をエミュレートして処理する．
そのため，ハードウェアアクセスによるオーバヘッドが発生し，準仮想化と比較してパフォーマンスが低下する傾向がある．

準仮想化はゲスト OS に変更を加え，VMM とゲスト OS 間を連携させて仮想化を実現する手法である\cite{barhamXenArtVirtualization}．
ゲスト OS を変更する必要があるため完全仮想化に比べて互換性が低く，使用できる OS が限られる．
しかし，VM 内での特権命令をハイパーコールを用いて VMM と直接通信を行うことで
ハードウェア操作を効率的に行うため，完全仮想化に比べてハードウェアアクセスの
オーバヘッドが削減され，高パフォーマンスを実現できる．

\begin{figure}[H]
  \centering
  % --- 左側の図 ---
  \begin{minipage}[t]{0.3\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figure/Full_virtualization.png}
    \caption{完全仮想化}
    \label{fig:Full_virtualization} % ラベル名は変えておく
  \end{minipage}
  \hspace{20mm} % 図と図の間に隙間を空ける
  % --- 右側の図 ---
  \begin{minipage}[t]{0.3\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figure/Para_virtualization.png}
    \caption{準仮想化}
    \label{fig:Para_virtualization} % ラベル名は変えておく
  \end{minipage}
\end{figure}

\subsection{QEMU/KVM}

\subsubsection*{KVM}
KVM (Kernel-based Virtual Machine) \cite{KVM} は Linux に組み込まれた，主に完全仮想化を実現するハイパーバイザ型の VMM である．
作成された VM は Linux プロセスとして管理されるため，スケジューラやメモリ管理などは Linux カーネル
の既存の機能を活用でき，top() や kill() などの既存のプロセス管理インフラストラクチャも
使用できる．KVMの下では，VM はデバイスノード (/dev/kvm) を開くことによって作成され，
Linux ユーザメモリと同様に，カーネルは不連続なページを割り当てて，独立したゲストアドレス空間を作成する\cite{kivityKvmLinuxVirtual2007}．
仮想 CPU (vCPU) はそれ自体がスケジューリングされるわけではなく，Linux プロセスの 1 つとして扱われる．

vCPU の実行フローを図\ref{fig:flowchart}に示す．
KVM は既存のユーザモードとカーネルモードに加え，
CPU の仮想化支援機能として追加されたゲストモードを加えた 3 重のループ構造で vCPU を動作させる．
まずユーザモードで，ゲストモードとして VM を実行するために ioctl() が発行されカーネルモードに遷移する.
カーネルは CPU をゲストモードに遷移させ，プロセッサはハードウェアレベルでゲスト OS のコードを直接実行する．
I/O 処理や割り込みが発生するとゲストモードを終了してカーネルモードに遷移し，
ゲストモードが中断した理由を解析する．ここで，カーネルで完結する処理であればカーネルモードで処理し，
実行後にゲストモードを再開する．
I/O 処理やシグナルの待機がある場合はユーザモードに遷移してエミュレート処理やイベント処理を行い，
処理が終了したら再び ioctl() を発行してゲストモードでの実行を再開する．
実際の構成においてユーザモードは QEMU，カーネルモードは KVM，ゲストモードは VM のコードが実行される．

\begin{figure}[H]
  \centering
  % figureフォルダの中のファイルを指定する
  \includegraphics[width=0.7\linewidth]{figure/flowchart.png}
  \caption{vCPU の実行フロー}
  \label{fig:flowchart}
\end{figure}

\subsubsection*{QEMU}
QEMU (Quick Emulator) \cite{QEMU} は汎用性の高いオープンソースのマシンエミュレータであり，
様々なホスト OS 上でターゲットとなる OS とそのアプリケーションを，変更を加えずに VM 内で実行することができる．
QEMU は VM の実行状態を停止させ，その状態の保存や復元を行うことができるため，
ある OS を別の OS で実行したりデバッグとして用いられる．
QEMU ではターゲット命令を翻訳ブロックごとに中間表現に変換してからホスト命令に変換を行い，
その結果をキャッシュすることでホスト CPU と異なる CPU でも高速なエミュレーションを実現している\cite{bellardQEMUFastPortable2005b}．

QEMU の動作概念図を図\ref{fig:QEMU}に示す．
QEMU はホスト OS 上でユーザプロセスとして動作して，
VM 内のゲスト OS に対して CPU やメモリ，I/O デバイスをソフトウェア的に提供する．
ゲスト OS がこれらの仮想デバイスにアクセスすると QEMU がそれをエミュレートし，
ホスト OS を介して物理ハードウェアにアクセスする．

\begin{figure}[H]
  \centering
  % figureフォルダの中のファイルを指定する
  \includegraphics[width=0.3\linewidth]{figure/QEMU.png}
  \caption{QEMU の動作概念図}
  \label{fig:QEMU}
\end{figure}

\subsubsection*{KVM と QEMU の連携}
QEMU は単体でも VMM として動作するが，vCPU をソフトウェアでエミュレートするためパフォーマンスが大幅に低下する．
そのため，KVM と組み合わせて使用することでパフォーマンスを向上させる．
具体的には，KVM は CPU の仮想化支援機能 を用いてコードを直接実行しつつ，
Linux カーネルの機能を活用して CPU のスケジューリングやメモリ管理を行うことで
効率的な動作を提供し，QEMU はユーザレベルで動作して I/O デバイスなどをエミュレートすることで
ゲスト OS に独立したハードウェア環境を提供し，効率的かつ汎用的な仮想化環境を実現する．

KVM と QEMU 連携時の動作概念図を図\ref{fig:KVM_QEMU}に示す．
CPU やメモリへのアクセスは KVM が介入することで物理ハードウェアで直接実行されるため，
オーバーヘッドが最小限に抑えられる．
対して，I/O デバイスへのアクセスはユーザ空間の QEMU がエミュレーションを行うことで，
多様なハードウェア構成をサポートする．


\begin{figure}[H]
  \centering
  % figureフォルダの中のファイルを指定する
  \includegraphics[width=0.3\linewidth]{figure/KVM_QEMU.png}
  \caption{KVMとQEMU連携時の動作概念図}
  \label{fig:KVM_QEMU}
\end{figure}


\section{仮想マシンライブ移送}
仮想マシンライブ移送は， VM を稼働しながらその実行の影響を最小限に抑えつつ別ホストに移送する技術であり，
負荷分散\cite{woodBlackboxGrayboxStrategies}や，耐障害性\cite{nagarajanProactiveFaultTolerance2007a}，
省電力\cite{dasLiteGreenSavingEnergy}，パフォーマンス向上\cite{bobroffDynamicPlacementVirtual2007}などに利用されている．
VM ライブ移送の概略図を図\ref{fig:live_migration}に示す．
ライブ移送におけるメモリ転送は移送元 VM が実行中にページを移送先に転送する Push フェーズ，
移送元 VM を停止してページを移送先 VM にコピーした後 VM の実行を再開する Stop-and-Copy フェーズ，
移送先で VM が再開後，移送元からページを転送する Pull フェーズの 3 つのフェーズに分けることができる．
そして VM ライブ移送を行う手法には，を組み合わせて
移送を行う Pre-copy 手法と，Stop-and-Copy フェーズと Pull フェーズを組み合わせて
移送を行う Post-copy 手法の 2 種類に大別される．

ライブ移送の性能に影響を与える主要な要因は VM に割り当てられたメモリサイズ，
ページが書き換えられる頻度（ダーティレート），およびネットワーク帯域幅である\cite{akoushPredictingPerformanceVirtual2010}．
共有ストレージ環境下での VM 移送において，転送データの大部分を占めるのはメモリであり，
メモリサイズが大きいと移送完了に必要な総転送量が増加するため，
移送手法に関係なく総移送時間に影響を及ぼす．
VM のダーティレートが高いと，メモリ更新速度が転送速度を上回るリスクを高め，
Pre-copy 手法において反復転送の収束を妨げるため，
総移送時間に影響を及ぼす決定的な要因となりうる．
ネットワーク帯域幅が狭いとメモリ転送に時間がかかり，
Pre-copy 手法では反復転送中のダーティページの蓄積により総移送時間が長期化する要因になる．
Post-copy 手法では移送時間の増加だけでなく，
移送先で実行中の VM で発生するネットワークを介したページフォルトが
アプリケーションの性能劣化を招く要因になる．

\begin{figure}[H]
  \centering
  % figureフォルダの中のファイルを指定する
  \includegraphics[width=0.8\linewidth]{figure/live_migration.png}
  \caption{VM ライブ移送}
  \label{fig:live_migration}
\end{figure}

\subsection{Pre-copy 手法}
Pre-copy 手法は反復的な Push フェーズと Stop-and-Copy フェーズを組み合わせることで，
ダウンタイムと総移送時間の両方を最小化するようバランスがとれた形で移送を行う手法である\cite{clarkLiveMigrationVirtuala}．
Pre-copy 手法による移送の概略図を図\ref{fig:Pre_copy}に示す．
移送の流れとしてはまず，移送元ホストから移送先ホストに移送要求が発行され，
移送先ホストで必要なリソースが利用可能であることを確認する．次に Push フェーズに入り，
最初のラウンドで全てのページを転送し，次のラウンドではその前のラウンドの処理中に
変更されたページの転送を反復的に行う．
残りのメモリ転送量が閾値まで小さくなるか反復回数が上限に達したら Stop-and-Copy フェーズに入り，
移送元の VM の実行を一時停止して CPU 状態と残りのメモリページが転送される．
ここの終了時点では移送元ホストと移送先ホストの両方に一貫した一時停止状態の VM が存在し，
障害が発生した場合には移送元ホストで再開される．そして，移送先ホストが一貫した状態の VM を受信したことを
移送元ホストに通知し，移送先ホストで VM の実行が再開される．

\begin{figure}[H]
  \centering
  % figureフォルダの中のファイルを指定する
  \includegraphics[width=0.8\linewidth]{figure/Pre_copy.png}
  \caption{Pre-copy 手法}
  \label{fig:Pre_copy}
\end{figure}

\subsection{Post-copy 手法}
Post-copy 手法は Stop-and-Copy フェーズと Pull フェーズを組み合わせることで，
Pre-copy 手法よりと少ないメモリ転送量で移送を行う手法である\cite{hinesPostcopyLiveMigration2009}．
Post-copy 手法による移送の概略図を図\ref{fig:Post_copy}に示す．
移送の流れとしてはまず，移送元ホスト上の VM の実行を停止し，最小限のデータだけを移送先ホストに転送する．
そして，移送先ホストで VM の実行を再開させ，Pull フェーズで残りのメモリページを移送先ホストに転送する．
この時，移送先 VM でページフォルトが発生した際にページの転送を行う Demand Paging や，
VM が移送先ホストで実行中にバックグラウンドで移送元からページを転送する Active Pushing ，
ページフォルトが発生した際に，ページの空間的局所性を利用してその周辺のページも転送する Prepaging 
などの方法で転送を行う．

\begin{figure}[H]
  \centering
  % figureフォルダの中のファイルを指定する
  \includegraphics[width=0.8\linewidth]{figure/Post_copy.png}
  \caption{Post-copy 手法}
  \label{fig:Post_copy}
\end{figure}

\subsection{各手法の特徴}
これら 2 つの手法はそれぞれ異なる設計思想に基づいており，それぞれのトレードオフを有している．
まず，各手法の利点について述べる．
Pre-copy 手法の利点は，耐障害性が優れていることである．移送管理に対して保守的なアプローチをとっているため，
移送中に移送先ホストで障害が発生した場合でも，移送元ホストが最新の VM を保持しており，
そこから復元が可能である．
Post-copy 手法の利点は，移送の即時性が高いことである．
Pre-copy 手法では反復的なメモリ転送が完了してから VM の実行権を移すため，
移送先ホストで VM が実行されるまでに時間を要する．
一方 Post-copy 手法では，VM の実行権を先に移送先ホストに移してからメモリ転送を行うため，
メモリ負荷が高い VM であっても移送先ホストに瞬時に再配置することができる．
また各メモリページが 1 回しか転送されないため，重複転送のオーバヘッドを回避することができるという利点もある．

次に各手法の欠点について述べる．
Pre-copy 手法は，反復的なコピーによってネットワークオーバヘッドが発生し，総移送時間も長期化してしまう\cite{zhangSurveyVirtualMachine2018}．
特に，ページへの書き込み速度 (ダーティレート) がメモリ転送速度を上回ると，反復転送が収束せず
同じページの再送を繰り返す無駄な転送処理が発生してしまう．
Post-copy 手法は移送先ホストでページフォルトによる遅延が頻発することが欠点として挙げられる．
さらに移送先ホストが最新の VM の状態を保持するため，移送中に移送先ホストで障害が発生すると
VM が壊れてしまうという致命的なリスクも伴う．

このように，両手法には信頼性と即時性でトレードオフが存在するため，
Pre-copy 手法は読み取り主体のワークロードを持つ VM の移送に適した手法であり，
Post-copy 手法は大容量メモリや書き込み主体のワークロードを持つ VM の移送に適した手法であると言える．


\section{ドキュメントデータベース}
\subsection{概要}
ドキュメントデータベース (DDB) とはキーと値のペアを JSON や JSON ライクな
ドキュメントで管理する，NoSQL データベースの一種である．
DDB では値をデータベースの解釈可能な JSON ドキュメントとして保存するため，
クエリの対象にすることができ，ネストされたような複雑なデータ構造もより便利に扱うことができる．
またスキーマの制限がないため，新しい属性を持つドキュメントも容易に追加できたりと柔軟な構造を持つ．
DDB はリレーショナルデータベースが持つ厳格な整合性管理などの機能を省き，
関連データを１つのドキュメントに集約する非正規化によって，
低レイテンシによるパフォーマンス向上を実現している\cite{akoushPredictingPerformanceVirtual2010} ．
また，データを複数サーバに分散保存する設計による水平方向のスケーラビリティが優れており，
ビッグデータ処理やクラウドコンピューティング環境に適している\cite{carvalhoPerformanceEvaluationNoSQL2023}．
最も一般的用途は，リアルタイム分析，ロギング，そしてブログのような小規模で柔軟な Web サイトの
ストレージ層である\cite{hechtNoSQLEvaluationUse2011} ．

\subsection{MongoDB}
代表的な DDB の 1 つに MongoDB \cite{MongoDB}がある．
データは JSON のバイナリ形式である BSON で効率的に保存される．
シャーディングを使用して，大規模なデータセットと高スループット操作を伴う配置をサポートする．
また，ドキュメントをサーバー間で分散させる自動シャーディングをサポートしている\cite{cattellScalableSQLNoSQL2011}．
MongoDB のデフォルトのストレージエンジンは WiredTiger であり，
ドキュメントレベルでの同時実行制御や，チェックポイント，圧縮などの機能を持つ．
WiredTiger は，256 MB かシステム RAM のサイズに 1GB 引いた値の 50\% の
いずれか大きい方のサイズを内部キャッシュとして使用する．例えば，5GB の RAM を持つシステムでは，
2GB の RAM をキャッシュとして使用する ((5GB - 1GB) / 2 = 2GB > 256MB)．
WiredTiger はテーブルを B+Tree として管理する．ルートページと内部ページには他のページの
ポインタが格納され，リーフページには実際のデータが格納される．
このデータ管理において，WiredTiger はリーフページの実データを，
ディスクから読み込んだディスクイメージと，
メモリ上での更新差分に分離して管理する MVCC (Multi-Version Concurrency Control：多版同時実行制御)を使用する．

\section{問題点}
MongoDB のような DDB を実行している VM の移送においては
メモリサイズとダーティレートが高くなる傾向にあり，移送を困難にしている．
DDB は応答速度の向上を目的として多くのデータをメモリ上にキャッシュするため，
肥大化したメモリ使用量が総移送時間の長期化を招く．
また，DDB は頻繁なデータ書き込みを行うためダーティレートが高く，
特に Pre-copy 手法を用いる場合，ダーティレートが転送速度を上回ると
メモリを転送してもダーティページが減らず，反復転送が収束しない状態に陥る．
その結果 Stop-and-Copy フェーズで転送すべきデータ量が削減されず，
ダウンタイム中に転送すべきページ量が増加してしまい，
総移送時間とダウンタイムの双方が大幅に増大してしまう．

このような移送の長期化によって生じる問題点を図\ref{fig:live_migration_problem}に示す．
まず，負荷分散の失敗が挙げられる．
移送に長時間を要すると，移送完了時にはホストの稼働状況が変化してしまい，
想定していた負荷分散の効果が得られなくなるリスクがある．
また，移送の長期化はサービス品質の劣化も招く．
移送処理自体がネットワーク帯域や CPU リソースを消費するため，
移送が長引けば DDB 自体のスループット低下やレイテンシ悪化を招く．
そのため，MongoDB のようなメモリ集約型アプリケーションにおける移送時間の短縮は，
解決すべき重要な課題である．

\begin{figure}[H]
  \centering
  % figureフォルダの中のファイルを指定する
  \includegraphics[width=0.9\linewidth]{figure/live_migration_problem.png}
  \caption{VM ライブ移送における問題点}
  \label{fig:live_migration_problem}
\end{figure}